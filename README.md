<p align="center">
  <h1 align="center">DepthSplat: Connecting Gaussian Splatting and Depth</h1>
  <p align="center">
    <a href="https://haofeixu.github.io/">Haofei Xu</a>
    Â·
    <a href="https://pengsongyou.github.io/">Songyou Peng</a>
    Â·
    <a href="https://fangjinhuawang.github.io/">Fangjinhua Wang</a>
    Â·
    <a href="https://hermannblum.net/">Hermann Blum</a>
    Â·
    <a href="https://scholar.google.com/citations?user=U9-D8DYAAAAJ">Daniel Barath</a>
    Â·
    <a href="http://www.cvlibs.net/">Andreas Geiger</a>
    Â·
    <a href="https://people.inf.ethz.ch/marc.pollefeys/">Marc Pollefeys</a>
  </p>
  <h3 align="center"><a href="https://arxiv.org/abs/2410.13862">Paper</a> | <a href="https://haofeixu.github.io/depthsplat/">Project Page</a> </h3>
  <div align="center"></div>
</p>
<p align="center">
  <a href="">
    <img src="https://haofeixu.github.io/depthsplat/assets/teaser.png" alt="Logo" width="100%">
  </a>
</p>


<p align="center">
<strong>DepthSplat enables cross-task interactions between Gaussian splatting and depth estimation.</strong>
</p>

## Directory Structure

```bash
depthsplat
â”œâ”€â”€ DATASETS.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ MODEL_ZOO.md
â”œâ”€â”€ README.md
â”œâ”€â”€ assets
â”‚   â”œâ”€â”€ dl3dv_start_0_distance_100_ctx_12v_tgt_16v_video.json
â”‚   â”œâ”€â”€ dl3dv_start_0_distance_10_ctx_2v_tgt_4v.json
â”‚   â”œâ”€â”€ dl3dv_start_0_distance_50_ctx_2v_tgt_4v_video_0-50.json
â”‚   â”œâ”€â”€ dl3dv_start_0_distance_50_ctx_4v_tgt_4v_video_0-50.json
â”‚   â”œâ”€â”€ dl3dv_start_0_distance_50_ctx_6v_tgt_8v.json
â”‚   â”œâ”€â”€ dl3dv_start_0_distance_50_ctx_6v_tgt_8v_video_0-50.json
â”‚   â”œâ”€â”€ evaluation_index_re10k.json
â”‚   â””â”€â”€ evaluation_index_re10k_video.json
â”œâ”€â”€ config
â”‚   â”œâ”€â”€ dataset
â”‚   â”‚   â”œâ”€â”€ dl3dv.yaml
â”‚   â”‚   â”œâ”€â”€ re10k.yaml
â”‚   â”‚   â”œâ”€â”€ view_sampler
â”‚   â”‚   â””â”€â”€ view_sampler_dataset_specific_config
â”‚   â”œâ”€â”€ experiment
â”‚   â”‚   â”œâ”€â”€ dl3dv.yaml
â”‚   â”‚   â””â”€â”€ re10k.yaml
â”‚   â”œâ”€â”€ loss
â”‚   â”‚   â”œâ”€â”€ lpips.yaml
â”‚   â”‚   â””â”€â”€ mse.yaml
â”‚   â”œâ”€â”€ main.yaml
â”‚   â””â”€â”€ model
â”‚       â”œâ”€â”€ decoder
â”‚       â””â”€â”€ encoder
â”œâ”€â”€ datasets
â”‚   â””â”€â”€ re10k -> /root/autodl-tmp/re10k
â”œâ”€â”€ download_dataset.py
â”œâ”€â”€ pretrained
â”‚   â”œâ”€â”€ depthsplat-gs-base-re10k-256x256-044fdb17.pth -> /root/autodl-tmp/models/depthsplat-gs-base-re10k-256x256-044fdb17.pth
â”‚   â”œâ”€â”€ depthsplat-gs-large-re10k-256x256-288d9b26.pth -> /root/autodl-tmp/models/depthsplat-gs-large-re10k-256x256-288d9b26.pth
â”‚   â””â”€â”€ depthsplat-gs-small-re10k-256x256-49b2d15c.pth -> /root/autodl-tmp/models/depthsplat-gs-small-re10k-256x256-49b2d15c.pth
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ scripts
â”‚   â”œâ”€â”€ dl3dv_256x448_depthsplat_base.sh
â”‚   â”œâ”€â”€ inference_depth_base.sh
â”‚   â”œâ”€â”€ inference_depth_large.sh
â”‚   â”œâ”€â”€ inference_depth_small.sh
â”‚   â”œâ”€â”€ re10k_256x256_depthsplat_base.sh
â”‚   â”œâ”€â”€ re10k_256x256_depthsplat_large.sh
â”‚   â””â”€â”€ re10k_256x256_depthsplat_small.sh
â””â”€â”€ src
    â”œâ”€â”€ __pycache__
    â”‚   â”œâ”€â”€ config.cpython-310.opt-jaxtyping983a4111806314cc973c4ea00fb072bf6.pyc
    â”‚   â”œâ”€â”€ global_cfg.cpython-310.opt-jaxtyping983a4111806314cc973c4ea00fb072bf6.pyc
    â”‚   â””â”€â”€ main.cpython-310.pyc
    â”œâ”€â”€ config.py
    â”œâ”€â”€ dataset
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ __pycache__
    â”‚   â”œâ”€â”€ data_module.py
    â”‚   â”œâ”€â”€ dataset.py
    â”‚   â”œâ”€â”€ dataset_dl3dv.py
    â”‚   â”œâ”€â”€ dataset_re10k.py
    â”‚   â”œâ”€â”€ shims
    â”‚   â”œâ”€â”€ types.py
    â”‚   â”œâ”€â”€ validation_wrapper.py
    â”‚   â””â”€â”€ view_sampler
    â”œâ”€â”€ evaluation
    â”‚   â”œâ”€â”€ __pycache__
    â”‚   â”œâ”€â”€ evaluation_cfg.py
    â”‚   â”œâ”€â”€ evaluation_index_generator.py
    â”‚   â”œâ”€â”€ metric_computer.py
    â”‚   â””â”€â”€ metrics.py
    â”œâ”€â”€ geometry
    â”‚   â”œâ”€â”€ __pycache__
    â”‚   â”œâ”€â”€ epipolar_lines.py
    â”‚   â””â”€â”€ projection.py
    â”œâ”€â”€ global_cfg.py
    â”œâ”€â”€ loss
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ __pycache__
    â”‚   â”œâ”€â”€ loss.py
    â”‚   â”œâ”€â”€ loss_lpips.py
    â”‚   â””â”€â”€ loss_mse.py
    â”œâ”€â”€ main.py
    â”œâ”€â”€ misc
    â”‚   â”œâ”€â”€ LocalLogger.py
    â”‚   â”œâ”€â”€ __pycache__
    â”‚   â”œâ”€â”€ benchmarker.py
    â”‚   â”œâ”€â”€ collation.py
    â”‚   â”œâ”€â”€ discrete_probability_distribution.py
    â”‚   â”œâ”€â”€ heterogeneous_pairings.py
    â”‚   â”œâ”€â”€ image_io.py
    â”‚   â”œâ”€â”€ nn_module_tools.py
    â”‚   â”œâ”€â”€ render_utils.py
    â”‚   â”œâ”€â”€ resume_ckpt.py
    â”‚   â”œâ”€â”€ sh_rotation.py
    â”‚   â”œâ”€â”€ stablize_camera.py
    â”‚   â”œâ”€â”€ step_tracker.py
    â”‚   â””â”€â”€ wandb_tools.py
    â”œâ”€â”€ model
    â”‚   â”œâ”€â”€ __pycache__
    â”‚   â”œâ”€â”€ decoder
    â”‚   â”œâ”€â”€ encoder
    â”‚   â”œâ”€â”€ model_wrapper.py
    â”‚   â”œâ”€â”€ ply_export.py
    â”‚   â””â”€â”€ types.py
    â”œâ”€â”€ scripts
    â”‚   â”œâ”€â”€ convert_dl3dv_test.py
    â”‚   â”œâ”€â”€ convert_dl3dv_train.py
    â”‚   â””â”€â”€ generate_dl3dv_index.py
    â””â”€â”€ visualization
        â”œâ”€â”€ __pycache__
        â”œâ”€â”€ annotation.py
        â”œâ”€â”€ camera_trajectory
        â”œâ”€â”€ color_map.py
        â”œâ”€â”€ colors.py
        â”œâ”€â”€ drawing
        â”œâ”€â”€ layout.py
        â”œâ”€â”€ validation_in_3d.py
        â””â”€â”€ vis_depth.py
```



## Installation

Our code is developed based on pytorch 2.4.0, CUDA 12.4 and python 3.10. 

We recommend using [conda](https://docs.anaconda.com/miniconda/) for installation:

```bash
conda create -y -n depthsplat python=3.10
conda activate depthsplat

pip install torch==2.4.0 torchvision==0.19.0 --index-url https://download.pytorch.org/whl/cu124
pip install xformers==0.0.27.post2
pip install -r requirements.txt
```

## Model Zoo

Our models are hosted on Hugging Face ðŸ¤— : https://huggingface.co/haofeixu/depthsplat

Model details can be found at [MODEL_ZOO.md](MODEL_ZOO.md).

We assume the downloaded weights are located in the `pretrained` directory.

## Camera Conventions

The camera intrinsic matrices are normalized (the first row is divided by image width, and the second row is divided by image height).

The camera extrinsic matrices are OpenCV-style camera-to-world matrices ( +X right, +Y down, +Z camera looks into the screen).

## Datasets

Please refer to [DATASETS.md](DATASETS.md) for dataset preparation.



## Depth Prediction

Please check [scripts/inference_depth_small.sh](scripts/inference_depth_small.sh), [scripts/inference_depth_base.sh](scripts/inference_depth_base.sh), and [scripts/inference_depth_large.sh](scripts/inference_depth_large.sh) for scale-consistent depth prediction with models of different sizes.

![depth](https://haofeixu.github.io/depthsplat/assets/depth/c37109a55effe0000f8e40652ca935376e75bcb2a0b56de8eabd20a26e2a0f68.png)



We plan to release a simple depth inference pipeline in [UniMatch repo](https://github.com/autonomousvision/unimatch).



## Gaussian Splatting

- The training, evaluation, and rendering scripts on RealEstate10K dataset are available at [scripts/re10k_256x256_depthsplat_small.sh](scripts/re10k_256x256_depthsplat_small.sh), [scripts/re10k_256x256_depthsplat_base.sh](scripts/re10k_256x256_depthsplat_base.sh), and [scripts/re10k_256x256_depthsplat_large.sh](scripts/re10k_256x256_depthsplat_large.sh).

- The training, evaluation, and rendering scripts on DL3DV dataset are available at [scripts/dl3dv_256x448_depthsplat_base.sh](scripts/dl3dv_256x448_depthsplat_base.sh).

- Before training, you need to download the pre-trained [Depth Anything V2](https://github.com/DepthAnything/Depth-Anything-V2) and [UniMatch](https://github.com/autonomousvision/unimatch) weights and set up your [wandb account](config/main.yaml) for logging.

```
wget https://huggingface.co/depth-anything/Depth-Anything-V2-Small/resolve/main/depth_anything_v2_vits.pth -P pretrained
wget https://s3.eu-central-1.amazonaws.com/avg-projects/unimatch/pretrained/gmflow-scale1-things-e9887eda.pth -P pretrained
```


## Citation

```
@article{xu2024depthsplat,
      title   = {DepthSplat: Connecting Gaussian Splatting and Depth},
      author  = {Xu, Haofei and Peng, Songyou and Wang, Fangjinhua and Blum, Hermann and Barath, Daniel and Geiger, Andreas and Pollefeys, Marc},
      journal = {arXiv preprint arXiv:2410.13862},
      year    = {2024}
    }
```



## Acknowledgements

This project is developed with several fantastic repos: [pixelSplat](https://github.com/dcharatan/pixelsplat), [MVSplat](https://github.com/donydchen/mvsplat), [UniMatch](https://github.com/autonomousvision/unimatch), [Depth Anything V2](https://github.com/DepthAnything/Depth-Anything-V2) and [DL3DV](https://github.com/DL3DV-10K/Dataset). We thank the original authors for their excellent work.

